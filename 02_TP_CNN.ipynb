{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"aS75L2rGYGkE"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import model_selection\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import layers\n","import seaborn as sns\n","sns.set_theme(style=\"darkgrid\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"u-eY5pxT86E2"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"Dd5iy64dYqbD"},"source":["# R√©seaux de neurones convolutifs pour la classification de documents\n","\n"," > ‚ÑπÔ∏è Inspir√© de :\n"," > - https://keras.io/examples/nlp/pretrained_word_embeddings/\n"," > - https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/\n","\n","<div class=\"alert alert-block alert-info\">\n","\n","ü•Ö **Objectifs**\n","\n","- Savoir utiliser `keras` pour faire de l'apprentissage supervis√© √† partir de documents avec des r√©seaux de neurones convolutifs\n","- Savoir pr√©traiter les donn√©es pour le r√©seau\n","- Utiliser des plongement pr√©-entra√Æn√©s\n","- R√©aliser une validation crois√©e pour l'apprentissage\n","- Interpr√©ter les r√©sultats obtenus\n","\n","üö® **Consignes**\n","\n","Les r√©ponses aux questions doivent √™tre donn√©es sur Moodle (Questionnaires \"R√©ponses aux questions du TP\"). Cela vous permettra d'obtenir un retour imm√©diat pour les questions ferm√©es.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"iorMXir5bcGh"},"source":["## 1. Chargement des donn√©es"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gGTILZn1b1BK"},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' n'est pas reconnu en tant que commande interne\n","ou externe, un programme exÔøΩcutable ou un fichier de commandes.\n"]}],"source":["!mkdir data\n","!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/winemag-fr_train.csv"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"92jiRstRcBPA"},"outputs":[],"source":["# Lecture du fichier CSV\n","wine_df = pd.read_csv(\"data/winemag-fr_train.csv\", sep=\",\", dtype={'description': 'object',\n","                                           'price': 'float64',\n","                                           'province': 'category',\n","                                           'variety': 'object'})"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ErLz9G_YuSAw"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classes : ['Alsace', 'Beaujolais', 'Bordeaux', 'Burgundy', 'Champagne', 'France_Other', 'Languedoc-Roussillon', 'Loire_Valley', 'Provence', 'Rh√¥ne_Valley', 'Southwest_France']\n","Nombre d'exemplaires : 8132\n"]}],"source":["# Liste des classes\n","class_names = sorted(wine_df.province.unique().categories.to_list())\n","print(\"Classes :\", class_names)\n","print(\"Nombre d'exemplaires :\", len(wine_df))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FGYQP_lIzJBP"},"outputs":[{"data":{"text/plain":["{'Alsace': 0,\n"," 'Beaujolais': 1,\n"," 'Bordeaux': 2,\n"," 'Burgundy': 3,\n"," 'Champagne': 4,\n"," 'France_Other': 5,\n"," 'Languedoc-Roussillon': 6,\n"," 'Loire_Valley': 7,\n"," 'Provence': 8,\n"," 'Rh√¥ne_Valley': 9,\n"," 'Southwest_France': 10}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# On associe √† chaque classe un identifiant unique\n","class_index = {class_names[i]:i for i in range(len(class_names))}\n","class_index"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"W1ReFEDadAgl"},"outputs":[],"source":["# On utilise uniquement la vari√©t√© et les descriptions comme donn√©es d'entr√©e\n","X_train_variety = wine_df.variety.str.split('_')\n","X_train = X_train_variety.str.join(' ') + ' ' + wine_df.description\n","# Les noms des classes sont remplac√©es par leur identifiant (un entier positif)\n","y_train = wine_df.province.map(class_index)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JazhsYbNvxhu"},"outputs":[{"data":{"text/plain":["0    Chardonnay Ripe plush pineapple laces through ...\n","1    Rh√¥ne-style Red Blend If you're going to enjoy...\n","2    Chardonnay A concentrated, hugely rich wine wi...\n","3    Bordeaux-style Red Blend Chosen from a selecti...\n","4    Bordeaux-style Red Blend Very much in the food...\n","dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_train.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"O_iXDMCvv22m"},"outputs":[{"data":{"text/plain":["0    3\n","1    6\n","2    3\n","3    2\n","4    2\n","Name: province, dtype: category\n","Categories (11, int64): [0, 1, 2, 3, ..., 7, 8, 9, 10]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y_train.head()"]},{"cell_type":"markdown","metadata":{"id":"p4Ahpb8xdLKZ"},"source":["## 2. Indexation du vocabulaire\n","\n","Le vocabulaire sera constitu√© par les 8 000 mots les plus fr√©quents et les documents seront tronqu√©s ou compl√©t√©s de mani√®re √† faire 50 tokens de long. La vectorisation est r√©alis√©e √† l'aide de [TextVectorization](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/) de la biblioth√®que [Keras](https://keras.io/). Keras permet de construire, entra√Æner et √©valuer diff√©rents types de r√©seaux de neurones."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"n9xam7ZddKlp"},"outputs":[],"source":["def get_vectorizer(documents, max_voc_size=8000, max_seq_length=50, batch_size=128):\n","  vectorizer = TextVectorization(max_tokens=max_voc_size, \n","                                 output_sequence_length=max_seq_length)\n","  # Cr√©ation du jeu de donn√©es √† partir de X_train et constitution de lots de 128 instances\n","  text_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n","  # Cr√©ation du vocabulaire √† partir des donn√©es d'entr√©e\n","  vectorizer.adapt(text_ds)\n","  return vectorizer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9h_Qx8RKwYCH"},"outputs":[],"source":["keras_vectorizer = get_vectorizer(X_train)"]},{"cell_type":"markdown","metadata":{"id":"VPLthMLvc5X-"},"source":["Vocabulaire obtenu :"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nTuTysVWrfxB"},"outputs":[{"name":"stdout","output_type":"stream","text":["8000\n"]}],"source":["voc = keras_vectorizer.get_vocabulary()\n","print(len(voc))"]},{"cell_type":"markdown","metadata":{"id":"iQ8emgcfnLe2"},"source":["Affichage des 5 items les plus fr√©quents dans le vocabulaire :"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"PwHMaO593c5m"},"outputs":[{"data":{"text/plain":["['', '[UNK]', 'and', 'the', 'a']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["voc[:5]"]},{"cell_type":"markdown","metadata":{"id":"OVMGT30uoPhO"},"source":["L'item 0 est r√©serv√© pour compl√©ter les s√©quences (cha√Æne vide). L'item 1 est r√©serv√© aux mots hors vocabulaire (UNK).\n","\n","On associe ensuite √† chaque item du vocabulaire un identifiant unique :"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Je27iqIIrpjp"},"outputs":[],"source":["word_index = dict(zip(voc, range(len(voc))))"]},{"cell_type":"markdown","metadata":{"id":"kV1DTq03nT5R"},"source":["Exemple de vectorisation d'une phrase"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ingFy9kMniT8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Texte initial Rh√¥ne-style Red Blend If you're going to enjoy this wine now, let it open up for a couple of hours to allow some of the nuanced notes to express themselves. Sultry aromas of boysenberry and blueberry are framed with complex additions of purple florals, sweet cured meat, vanilla bean and toasty oak. The palate is lush and mouthfilling, with fine but aggressive tannins and a beautiful minerality to the dark fruit core. Cocoa-dusted truffle flavors flood the long finish; hold until 2016‚Äì2020.\n","Vocabulaire dans le texte (15 premiers items) :\n","101 rh√¥nestyle\n","15 red\n","12 blend\n","364 if\n","2858 youre\n","442 going\n","11 to\n","784 enjoy\n","9 this\n","7 wine\n","47 now\n","1279 let\n","10 it\n","186 open\n","140 up\n"]}],"source":["print(\"Texte initial\", X_train.iloc[1])\n","output = keras_vectorizer([X_train.iloc[1]])\n","print(\"Vocabulaire dans le texte (15 premiers items) :\")\n","for v in output.numpy()[0, :15]:\n","  print(v, keras_vectorizer.get_vocabulary()[v])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sG9v-0Ggob13"},"source":["## 3. Chargement de plongements de mots pr√©-entra√Æn√©s\n","\n","Nous allons utiliser des plongement de mots pr√©-entra√Æn√©s pour repr√©senter les mots du vocabulaire. \n","\n","Nous allons commencer par tester des plongements obtenus √† partir de Wikipedia en anglais, d'une dimension de 300 et construits avec l'algorithme \"Continuous Skipgram\" de Gensim (cf. https://git.unistra.fr/dbernhard/ftaa_data/-/blob/main/README.md)\n","\n","üö® **Les plongements utilis√©s d√©pendent de la langue des textes. Ces plongements ne sont donc pas adapt√©s pour des textes dans une langue autre que l'anglais.** üö® \n","\n","T√©l√©chargement des donn√©es :"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"uP2iPaXmp29V"},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' n'est pas reconnu en tant que commande interne\n","ou externe, un programme exÔøΩcutable ou un fichier de commandes.\n"]}],"source":["!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/model_6.txt"]},{"cell_type":"markdown","metadata":{"id":"LuxBYJrOe1Qb"},"source":["Nous allons construire un dictionnaire qui associera chaque mot √† sa repr√©sentation vectorielle :"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"uD3eYGzPrBuB"},"outputs":[],"source":["def load_embeddings(embeddings_file):\n","  embeddings_index = {}\n","  with open(embeddings_file, 'r', encoding='utf8') as f:\n","      for line in f:\n","          word, coefs = line.split(maxsplit=1)\n","          coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","          embeddings_index[word] = coefs\n","  print(f'{len(embeddings_index)} vecteurs de mots ont √©t√© lus')\n","  return embeddings_index"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"0QPQWmolxUcJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["6737 vecteurs de mots ont √©t√© lus\n"]}],"source":["# Chargement des plongements du fichier model_6.txt\n","m6_embeddings = load_embeddings('model_6.txt')"]},{"cell_type":"markdown","metadata":{"id":"haJAXEQgfOU_"},"source":["Pour information, voici √† quoi ressemble le fichier contenant les plongements pr√©-entra√Æn√©s :"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Bma20sVvrxz8"},"outputs":[{"name":"stderr","output_type":"stream","text":["'head' n'est pas reconnu en tant que commande interne\n","ou externe, un programme exÔøΩcutable ou un fichier de commandes.\n"]}],"source":["!head model_6.txt"]},{"cell_type":"markdown","metadata":{"id":"-4OIyo32fgoJ"},"source":["Nous allons maintenant pr√©parer une matrice de plongements. Dans cette matrice, la ligne `i` correspondra au plongement pr√©-entra√Æn√© pour le mot d'indice `i` dans le vocabulaire :"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"inAbw5W1rXFx"},"outputs":[],"source":["def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 300):\n","  num_tokens = len(vocabulary)\n","  hits = 0\n","  misses = 0\n","\n","  # Pr√©paration de la matrice\n","  # Les mots qui ne se trouvent pas dans les plongements pr√©-entra√Æn√©s seront \n","  # repr√©sent√©s par des vecteurs dont toutes les composantes sont √©gales √† 0,\n","  # y compris la repr√©sentation utilis√©e pour compl√©ter les documents courts et\n","  # celle utilis√©e pour les mots inconnus [UNK]\n","  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","  for word, i in word_index.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:\n","          embedding_matrix[i] = embedding_vector\n","          hits += 1\n","      else:\n","          misses += 1\n","  print(f'{hits} mots ont √©t√© trouv√©s dans les plongements pr√©-entra√Æn√©s')\n","  print(f'{misses} sont absents')\n","  return embedding_matrix"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"2OEWxdkYx3GW"},"outputs":[{"name":"stdout","output_type":"stream","text":["5281 mots ont √©t√© trouv√©s dans les plongements pr√©-entra√Æn√©s\n","2719 sont absents\n"]}],"source":["# Construction de la matrice de plongements √† partir du vocabulaire\n","m6_embedding_matrix = get_embedding_matrix(voc, m6_embeddings)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Ax8cah7q-Hw1"},"outputs":[{"data":{"text/plain":["(8000, 300)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["m6_embedding_matrix.shape"]},{"cell_type":"markdown","metadata":{"id":"M0DJAj8PsnHl"},"source":["## 4. Construction et entra√Ænement du mod√®le\n","\n","\n","Nous allons faire une validation crois√©e √† 5 plis.\n","\n","L'architecture du mod√®le est la suivante :    \n","- Entr√©e : documents repr√©sent√©s par la concat√©nation des plongements repr√©sentant les mots\n","  - Les donn√©es sont trait√©es par lots de 128 documents (`batch_size`) : la descente du gradient se fera sur un lot.\n","  - La repr√©sentation de chaque document a une longueur de 50 tokens (avec remplissage si n√©cessaire)\n","  - Chaque token est repr√©sent√© par un plongement de dimension 300.\n","- Couche 1 : convolution avec 64 filtres et une fen√™tre de 5 mots\n","  - Pour chacun des 64 filtres, on obtient une carte de 46 caract√©ristiques (dans 50 tokens, il y a 46 fen√™tres de 5 tokens)\n","  - Nombre de param√®tres √† apprendre (300 * 5 + 1 ) * 64 = 96064\n","- Couche 2 : pooling maximum avec une fen√™tre de 5 et un pas de 5 :    \n","  - Pour chaque carte de caract√©ristiques, on obtient un vecteur de dimension 9 (il y a 9 fen√™tres de taille 5 qui ne se superposent pas dans un vecteur de 46 caract√©ristiques)\n","- Couche 3 : convolution avec 64 filtres et une fen√™tre de 5\n","  - Pour chaque filtre, on obtient une carte de 5 caract√©ristiques (5 fen√™tres de 5 caract√©ristiques dans un vecteur de 9 caract√©ristiques)\n","  - Nombre de param√®tres √† apprendre : (64 * 5 + 1 ) * 64 = 20544\n","- Couche 4 : pooling maximum global. \n","  - On ne conserve que le maximum de chaque carte de caract√©ristiques\n","- Couche 5 : couche enti√®rement connect√©e avec 64 unit√©s\n","  - Nombre de param√®tres √† apprendre : 64 * (64 + 1) = 4160\n","- Couche 6 : _Dropout_ \n","  - Pour √©viter le surajustement, 50% des neurones sont al√©atoirement ignor√©s. De sorte, les neurones sont \"forc√©s\" √† (1) avoir chacun leur propre utilit√©, car ils ne peuvent pas s'adapter avec leurs neurones voisins, et (2) prendre en compte tous les neurones d'entr√©e, et non pas se focaliser sur quelques uns.\n","- Couche 7 : couche enti√®rement connect√©e avec 11 unit√©s (le nombre de classes)\n","  - Retourne une liste de 11 probabilit√©s (une pour chacune des classes cibles)\n","  - Nombre de param√®tres √† apprendre : 11 * (64 + 1) = 715"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"8yjEq0ErykRe"},"outputs":[],"source":["def get_CNN_model(voc_size, embedding_matrix, embedding_dim=300):\n","  # Cr√©ation du mod√®le\n","  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n","  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n","      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","  )\n","  \n","  embedded_sequences = embedding_layer(int_sequences_input)\n","  x = layers.Conv1D(64, 5, activation=\"relu\")(embedded_sequences)\n","  x = layers.MaxPooling1D(5)(x)\n","  x = layers.Conv1D(64, 5, activation=\"relu\")(x)\n","  x = layers.GlobalMaxPooling1D()(x)\n","  x = layers.Dense(64, activation=\"relu\")(x)\n","  x = layers.Dropout(0.5)(x)\n","  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n","  model = keras.Model(int_sequences_input, preds)\n","  return model"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"smpHUUqwzf5j"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 300)         2400000   \n","                                                                 \n"," conv1d (Conv1D)             (None, None, 64)          96064     \n","                                                                 \n"," max_pooling1d (MaxPooling1D  (None, None, 64)         0         \n"," )                                                               \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, None, 64)          20544     \n","                                                                 \n"," global_max_pooling1d (Globa  (None, 64)               0         \n"," lMaxPooling1D)                                                  \n","                                                                 \n"," dense (Dense)               (None, 64)                4160      \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 11)                715       \n","                                                                 \n","=================================================================\n","Total params: 2,521,483\n","Trainable params: 2,521,483\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Affichage de l'architecture du mod√®le\n","m6_model = get_CNN_model(len(voc), m6_embedding_matrix)\n","m6_model.summary()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"rQD_IZTzcsjB"},"outputs":[],"source":["# Fonction pour l'entra√Ænement d'un mod√®le\n","def train_model(X, y, model_function, vectorizer,\n","                voc_size, embedding_matrix, embedding_dim=300, batch_size=128):\n","  # Listes utilis√©es pour sauvegarder les r√©sultats obtenus √† chaque pli\n","  acc_per_fold = []\n","  loss_per_fold = []\n","  histories = []\n","  folds = 5\n","  stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, \n","                                              random_state=12)\n","  fold_no = 1\n","  for train, test in stratkfold.split(X, y):\n","    m_function = globals()[model_function]\n","    model = m_function(voc_size, embedding_matrix, embedding_dim)\n","\n","    print('------------------------------------------------------------------------')\n","    print(f'Entra√Ænement pour le pli {fold_no} ...')\n","    fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n","    fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n","    fold_y_train = y.iloc[train].to_numpy()\n","    fold_y_val = y.iloc[test].to_numpy()\n","\n","    # Compilation du mod√®le : permet de pr√©ciser la fonction de perte et l'optimiseur\n","    # loss=sparse_categorical_crossentropy : entropie crois√©e, dans le cas o√π les \n","    #  classes cibles sont indiqu√©es sous forme d'entiers. Il s'agira de minimiser\n","    #  la perte pendant l'apprentissage\n","    # optimizer=rmsprop : l'optimiseur d√©termine la mani√®re doit les poids seront\n","    #  mis √† jour pendant l'apprentissage\n","    model.compile(\n","      loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n","    )\n","    # Entra√Ænement sur 10 √©poque (la totalit√© du jeu de donn√©es est parcourue\n","    # 10 fois)\n","    history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size, \n","                        epochs=10, validation_data=(fold_x_val, fold_y_val))\n","    histories.append(history)\n","    # Evaluation sur les donn√©es de validation\n","    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n","    print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n","          f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n","    acc_per_fold.append(scores[1] * 100)\n","    loss_per_fold.append(scores[0])\n","    fold_no = fold_no + 1\n","\n","  # Affichage des scores moyens par pli\n","  print('---------------------------------------------------------------------')\n","  print('Scores par pli')\n","  for i in range(0, len(acc_per_fold)):\n","    print('---------------------------------------------------------------------')\n","    print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n","          f'- Accuracy: {acc_per_fold[i]:.2f}%')\n","  print('---------------------------------------------------------------------')\n","  print('Scores moyens pour tous les plis :')\n","  print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n","        f'(+- {np.std(acc_per_fold):.2f})')\n","  print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n","  print('---------------------------------------------------------------------')\n","  return histories"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Y1t44LY3Akyh"},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------------------------------------------------\n","Entra√Ænement pour le pli 1 ...\n","Epoch 1/10\n","51/51 [==============================] - 3s 43ms/step - loss: 2.1999 - acc: 0.2589 - val_loss: 1.9305 - val_acc: 0.4216\n","Epoch 2/10\n","51/51 [==============================] - 2s 40ms/step - loss: 1.6426 - acc: 0.4732 - val_loss: 1.3412 - val_acc: 0.5519\n","Epoch 3/10\n","51/51 [==============================] - 2s 41ms/step - loss: 1.2715 - acc: 0.6008 - val_loss: 1.0633 - val_acc: 0.6632\n","Epoch 4/10\n","51/51 [==============================] - 2s 43ms/step - loss: 1.0585 - acc: 0.6692 - val_loss: 0.9040 - val_acc: 0.7154\n","Epoch 5/10\n","51/51 [==============================] - 2s 41ms/step - loss: 0.8770 - acc: 0.7368 - val_loss: 0.7052 - val_acc: 0.7947\n","Epoch 6/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.7445 - acc: 0.7745 - val_loss: 0.6375 - val_acc: 0.8095\n","Epoch 7/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.6139 - acc: 0.8146 - val_loss: 0.5852 - val_acc: 0.8377\n","Epoch 8/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.5215 - acc: 0.8506 - val_loss: 0.5470 - val_acc: 0.8482\n","Epoch 9/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.4446 - acc: 0.8650 - val_loss: 0.5151 - val_acc: 0.8513\n","Epoch 10/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.3676 - acc: 0.8922 - val_loss: 0.5136 - val_acc: 0.8543\n","Scores pour le pli 1: loss = 0.51; acc = 85.43%\n","------------------------------------------------------------------------\n","Entra√Ænement pour le pli 2 ...\n","Epoch 1/10\n","51/51 [==============================] - 3s 41ms/step - loss: 2.1659 - acc: 0.2604 - val_loss: 1.8420 - val_acc: 0.4014\n","Epoch 2/10\n","51/51 [==============================] - 2s 40ms/step - loss: 1.5907 - acc: 0.4849 - val_loss: 1.2237 - val_acc: 0.6152\n","Epoch 3/10\n","51/51 [==============================] - 2s 40ms/step - loss: 1.1732 - acc: 0.6161 - val_loss: 0.9769 - val_acc: 0.6951\n","Epoch 4/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.9484 - acc: 0.6955 - val_loss: 0.7922 - val_acc: 0.7671\n","Epoch 5/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.7642 - acc: 0.7711 - val_loss: 0.6765 - val_acc: 0.8125\n","Epoch 6/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.6375 - acc: 0.8178 - val_loss: 0.6428 - val_acc: 0.8242\n","Epoch 7/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.5356 - acc: 0.8464 - val_loss: 0.5908 - val_acc: 0.8408\n","Epoch 8/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.4694 - acc: 0.8709 - val_loss: 0.5474 - val_acc: 0.8531\n","Epoch 9/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.3834 - acc: 0.8930 - val_loss: 0.5465 - val_acc: 0.8420\n","Epoch 10/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.3161 - acc: 0.9096 - val_loss: 0.6892 - val_acc: 0.8095\n","Scores pour le pli 2: loss = 0.69; acc = 80.95%\n","------------------------------------------------------------------------\n","Entra√Ænement pour le pli 3 ...\n","Epoch 1/10\n","51/51 [==============================] - 3s 41ms/step - loss: 2.1658 - acc: 0.2816 - val_loss: 1.7958 - val_acc: 0.4059\n","Epoch 2/10\n","51/51 [==============================] - 2s 38ms/step - loss: 1.5882 - acc: 0.4843 - val_loss: 1.3056 - val_acc: 0.5984\n","Epoch 3/10\n","51/51 [==============================] - 2s 40ms/step - loss: 1.2410 - acc: 0.5990 - val_loss: 1.0016 - val_acc: 0.7109\n","Epoch 4/10\n","51/51 [==============================] - 2s 41ms/step - loss: 0.9816 - acc: 0.6858 - val_loss: 0.8182 - val_acc: 0.7620\n","Epoch 5/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.7893 - acc: 0.7556 - val_loss: 0.7820 - val_acc: 0.7737\n","Epoch 6/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.6650 - acc: 0.7983 - val_loss: 0.6433 - val_acc: 0.8204\n","Epoch 7/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.5531 - acc: 0.8372 - val_loss: 0.6149 - val_acc: 0.8364\n","Epoch 8/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.4587 - acc: 0.8655 - val_loss: 0.6161 - val_acc: 0.8333\n","Epoch 9/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.3874 - acc: 0.8798 - val_loss: 0.6736 - val_acc: 0.8093\n","Epoch 10/10\n","51/51 [==============================] - 2s 42ms/step - loss: 0.3081 - acc: 0.9079 - val_loss: 0.6040 - val_acc: 0.8364\n","Scores pour le pli 3: loss = 0.60; acc = 83.64%\n","------------------------------------------------------------------------\n","Entra√Ænement pour le pli 4 ...\n","Epoch 1/10\n","51/51 [==============================] - 3s 41ms/step - loss: 2.2053 - acc: 0.2445 - val_loss: 1.8578 - val_acc: 0.4090\n","Epoch 2/10\n","51/51 [==============================] - 2s 39ms/step - loss: 1.5994 - acc: 0.4871 - val_loss: 1.2615 - val_acc: 0.6089\n","Epoch 3/10\n","51/51 [==============================] - 2s 39ms/step - loss: 1.2257 - acc: 0.6107 - val_loss: 1.0107 - val_acc: 0.6870\n","Epoch 4/10\n","51/51 [==============================] - 2s 40ms/step - loss: 1.0213 - acc: 0.6809 - val_loss: 0.8557 - val_acc: 0.7478\n","Epoch 5/10\n","51/51 [==============================] - 2s 39ms/step - loss: 0.8509 - acc: 0.7270 - val_loss: 0.7204 - val_acc: 0.7823\n","Epoch 6/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.6990 - acc: 0.7811 - val_loss: 0.6315 - val_acc: 0.8216\n","Epoch 7/10\n","51/51 [==============================] - 2s 38ms/step - loss: 0.6019 - acc: 0.8140 - val_loss: 0.5850 - val_acc: 0.8383\n","Epoch 8/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.5073 - acc: 0.8521 - val_loss: 0.5487 - val_acc: 0.8512\n","Epoch 9/10\n","51/51 [==============================] - 2s 41ms/step - loss: 0.4234 - acc: 0.8809 - val_loss: 0.5615 - val_acc: 0.8549\n","Epoch 10/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.3428 - acc: 0.9036 - val_loss: 0.5457 - val_acc: 0.8647\n","Scores pour le pli 4: loss = 0.55; acc = 86.47%\n","------------------------------------------------------------------------\n","Entra√Ænement pour le pli 5 ...\n","Epoch 1/10\n","51/51 [==============================] - 2s 41ms/step - loss: 2.1396 - acc: 0.2744 - val_loss: 1.8225 - val_acc: 0.4071\n","Epoch 2/10\n","51/51 [==============================] - 2s 39ms/step - loss: 1.6068 - acc: 0.4865 - val_loss: 1.3341 - val_acc: 0.5959\n","Epoch 3/10\n","51/51 [==============================] - 2s 39ms/step - loss: 1.2725 - acc: 0.6024 - val_loss: 1.0672 - val_acc: 0.6765\n","Epoch 4/10\n","51/51 [==============================] - 2s 39ms/step - loss: 1.0413 - acc: 0.6678 - val_loss: 0.8915 - val_acc: 0.7355\n","Epoch 5/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.8437 - acc: 0.7395 - val_loss: 0.7858 - val_acc: 0.7780\n","Epoch 6/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.7176 - acc: 0.7891 - val_loss: 0.6961 - val_acc: 0.8026\n","Epoch 7/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.6246 - acc: 0.8108 - val_loss: 0.6460 - val_acc: 0.8247\n","Epoch 8/10\n","51/51 [==============================] - 2s 40ms/step - loss: 0.5174 - acc: 0.8495 - val_loss: 0.6311 - val_acc: 0.8235\n","Epoch 9/10\n","51/51 [==============================] - 2s 44ms/step - loss: 0.4545 - acc: 0.8686 - val_loss: 0.5965 - val_acc: 0.8370\n","Epoch 10/10\n","51/51 [==============================] - 2s 48ms/step - loss: 0.3664 - acc: 0.8895 - val_loss: 0.5691 - val_acc: 0.8444\n","Scores pour le pli 5: loss = 0.57; acc = 84.44%\n","---------------------------------------------------------------------\n","Scores par pli\n","---------------------------------------------------------------------\n","> Pli 1 - Loss: 0.51 - Accuracy: 85.43%\n","---------------------------------------------------------------------\n","> Pli 2 - Loss: 0.69 - Accuracy: 80.95%\n","---------------------------------------------------------------------\n","> Pli 3 - Loss: 0.60 - Accuracy: 83.64%\n","---------------------------------------------------------------------\n","> Pli 4 - Loss: 0.55 - Accuracy: 86.47%\n","---------------------------------------------------------------------\n","> Pli 5 - Loss: 0.57 - Accuracy: 84.44%\n","---------------------------------------------------------------------\n","Scores moyens pour tous les plis :\n","> Accuracy: 84.19 (+- 1.88)\n","> Loss: 0.58\n","---------------------------------------------------------------------\n"]}],"source":["# Entra√Ænement du mod√®le et r√©cup√©ration des r√©sultats\n","CNN_histories = train_model(X_train, y_train, 'get_CNN_model',\n","                            keras_vectorizer, len(voc), m6_embedding_matrix)"]},{"cell_type":"markdown","metadata":{"id":"7imQ26Is9JGL"},"source":["Affichage des r√©sultats sous forme graphique :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lEJ4lW49DQr"},"outputs":[],"source":["def plot_results(histories):\n","  accuracy_data = []\n","  loss_data = []\n","  for i, h in enumerate(histories):\n","    acc = h.history['acc']\n","    val_acc = h.history['val_acc']\n","    loss = h.history['loss']\n","    val_loss = h.history['val_loss']\n","    for j in range(len(acc)):\n","      accuracy_data.append([i+1, j+1, acc[j], 'Entra√Ænement'])\n","      accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n","      loss_data.append([i+1, j+1, loss[j], 'Entra√Ænement'])\n","      loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n","\n","  acc_df = pd.DataFrame(accuracy_data, \n","                        columns=['Pli', 'Epoch', 'Accuracy', 'Donn√©es'])\n","  sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Donn√©es',\n","              kind='line')\n","    \n","  loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Donn√©es'])\n","  sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Donn√©es',\n","              kind='line')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySo3Hei2CuLh"},"outputs":[],"source":["plot_results(CNN_histories)"]},{"cell_type":"markdown","metadata":{"id":"8U1d89XCL4aX"},"source":["La perte observ√©e sur les donn√©es d'entra√Ænement diminue √† chaque √©poque, tandis que la justesse augmente. On n'observe toutefois pas la m√™me tendance pour les donn√©es de validation : la perte et la justesse semblent stagner au bout de la 6<sup>√®me</sup> √©poque. Ainsi, un mod√®le qui obtient de meilleures performances sur les donn√©es d'entra√Ænement n'obtient pas forc√©ment de bons r√©sultats sur des donn√©es qui n'ont pas √©t√© utilis√©es pour l'apprentissage. Cela montre que le mod√®le est _surajust√©_ aux donn√©es d'entra√Ænement : on devrait donc stopper l'apprentissage au bout de 6 √©poques."]},{"cell_type":"markdown","metadata":{"id":"V0SyRAvzcsZJ"},"source":["‚ùì [1] Que constatez-vous par rapport aux r√©sultats obtenus pr√©c√©demment pour ce jeu de donn√©es (tf-idf) ?"]},{"cell_type":"markdown","metadata":{"id":"v6lCZVGIaP-n"},"source":["## 5. Comparaison de diff√©rents plongements\n","\n","Nous avons pour l'heure utilis√© un seul type de plongements (`model_6.txt`). Vous trouverez deux autres fichiers de plongements pr√©-entrain√©s dans le [d√©p√¥t de donn√©es](https://git.unistra.fr/dbernhard/ftaa_data) :      \n","- `model_26.txt`\n","- `glove_100.txt` (attention, ces derniers ont une dimension de 100 et non de 300, il faudra donc veiller √† utiliser les bons param√®tres pour les fonctions)\n","\n","Refaites l'exp√©rience en utilisant chacun de ces deux mod√®les. Pensez √† bien sauvegarder les r√©sultats pr√©c√©dents et veillez √† r√©utiliser les fonctions existantes :\n","- `load_embeddings()` pour charger les plongements\n","- `get_embedding_matrix()` pour construire la matrice de plongements\n","- `train_model()` pour entra√Æner le mod√®le\n","- `plot_results()` pour afficher les r√©sultats\n","\n","‚ùì [2] Que constatez-vous ? Les plongements utilis√©s ont-ils une influence sur les r√©sultats ?"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNbkGW5NqcYUZ6Cok7CYhvb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
