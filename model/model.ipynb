{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the data\n",
    "\n",
    "Now that we have a dataframe with 3 columns :  `tokenized` (title + synopsis), `genre` and `length`, we can vectorize the data. We will use the `Bag-of-Words` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elmah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../preprocessing')\n",
    "\n",
    "from process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/allocine_genres_train.csv')\n",
    "\n",
    "\n",
    "df_train = process_df(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by vectorizing the data using TF-IDF. TF-IDF stands for `Term Frequency - Inverse Document Frequency`. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train['synopsis_title'], df_train['genre'], test_size=0.2, random_state=42)\n",
    "\n",
    "models = []\n",
    "accuracy_scores = []\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,1), max_df=0.85,min_df=0.01,lowercase=False)\n",
    "tfid_X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "tfid_X_test = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"DecisionTreeClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"DecisionTreeClassifier\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"MultinomialNB accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"MultinomialNB\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"LogisticRegression accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"LogisticRegression\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"RandomForestClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"RandomForestClassifier\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"AdaBoostClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"AdaBoostClassifier\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"GradientBoostingClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"GradientBoostingClassifier\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"SGDClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"SGDClassifier\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = SVC(random_state=42)\n",
    "clf.fit(tfid_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(tfid_X_test)\n",
    "\n",
    "print(\"SVC accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models.append(\"SVC\")\n",
    "accuracy_scores.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of each model\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(models, accuracy_scores)\n",
    "plt.title(\"Accuracy of each model\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Logistic Regression has the best score with TF-IDF. We will use this model to predict the genre of the movies.But first, we will try to improve the score by using a different vectorizer like Bag-of-Words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "\n",
    "models_bow = []\n",
    "accuracy_scores_bow = []\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range = (1,1), max_df=0.85,min_df=0.01,lowercase=False)\n",
    "bow_X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "bow_X_test = vectorizer.transform(X_test)\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"DecisionTreeClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"DecisionTreeClassifier\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"MultinomialNB accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"MultinomialNB\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"LogisticRegression accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"LogisticRegression\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"RandomForestClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"RandomForestClassifier\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"AdaBoostClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"AdaBoostClassifier\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"GradientBoostingClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"GradientBoostingClassifier\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"SGDClassifier accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"SGDClassifier\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(random_state=42)\n",
    "clf.fit(bow_X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(bow_X_test)\n",
    "\n",
    "print(\"SVC accuracy : \", accuracy_score(y_test, y_pred))\n",
    "models_bow.append(\"SVC\")\n",
    "accuracy_scores_bow.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of each model and compare with tf-idf\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(models, accuracy_scores, label=\"tf-idf\")\n",
    "plt.bar(models_bow, accuracy_scores_bow, label=\"bag of words\")\n",
    "plt.title(\"Accuracy of each model\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Multinomial Naive Bayes has the best score with Bag-of-Words. We will use this model to predict the genre of the movies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test our model on our test set using the Bag-of-Words vectorizer with Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with new data using our best model, bow with MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(bow_X_train, y_train)\n",
    "\n",
    "# # save the model\n",
    "# pickle.dump(clf, open(\"model.pkl\", \"wb\"))\n",
    "# pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = pickle.load(open(\"trained_model/model.pkl\", \"rb\"))\n",
    "vectorizer = pickle.load(open(\"trained_model/vectorizer.pkl\", \"rb\"))\n",
    "\n",
    "# test with new data\n",
    "df_test = pd.read_csv('../data/allocine_genres_test.csv')\n",
    "df_test = process_df(df_test)\n",
    "\n",
    "X_test = df_test[\"synopsis_title\"]\n",
    "y_test = df_test[\"genre\"]\n",
    "\n",
    "bow_X_test = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(bow_X_test)\n",
    "\n",
    "print(\"MultinomialNB accuracy : \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# lets try with a new synopsis\n",
    "synopsis = \"Dans la ville de New York, le détective John Smith est sur la piste d'un tueur en série qui terrorise la ville depuis plusieurs mois. Alors que l'enquête piétine, le meurtrier continue de sévir et les victimes s'accumulent. John, qui est hanté par ses propres démons, est déterminé à arrêter le tueur avant qu'il ne frappe à nouveau. Au fil de l'enquête, John rencontre plusieurs personnages suspects, dont une ex-petite amie du tueur et un membre influent de la mafia locale. Il doit naviguer dans un monde sombre et dangereux pour découvrir la vérité sur l'identité du tueur. Alors que les indices s'accumulent et que la pression monte, John doit faire face à ses propres démons intérieurs et aux doutes de ses collègues. Dans une course contre la montre, il doit résoudre le mystère avant qu'il ne soit trop tard et que le tueur ne frappe de nouveau. Le film est rempli de suspense, de tension et de rebondissements jusqu'à son dénouement final choquant.\"\n",
    "titre = \"Au coeur de la nuit\"\n",
    "\n",
    "synopsis_2 = \"Nous suivons Marie Curie dès ses débuts en Pologne, alors qu'elle doit lutter contre les préjugés de genre pour poursuivre ses études scientifiques. Elle rencontre ensuite Pierre Curie, un physicien français, et ils tombent amoureux. Ensemble, ils commencent à travailler sur la radioactivité et découvrent le radium et le polonium, pour lesquels ils reçoivent le prix Nobel de physique en 1903. Mais leur travail n'a pas été facile et ils ont dû surmonter de nombreux obstacles, notamment les préjugés de genre et les réticences de la communauté scientifique. Ils ont également dû faire face à des tragédies personnelles, notamment la mort prématurée de Pierre. Après la mort de Pierre, Marie poursuit ses recherches et devient la première femme professeure à la Sorbonne. Elle reçoit également un deuxième prix Nobel, cette fois en chimie, pour son travail sur les éléments radioactifs. Le film montre comment Marie Curie a changé la face de la science et a ouvert la voie à d'autres femmes scientifiques. Mais il montre également les sacrifices qu'elle a dû faire pour y parvenir, notamment la lutte pour concilier son travail et sa vie de famille. Le film est une célébration de la vie extraordinaire de Marie Curie, une femme qui a brisé les barrières et ouvert la voie à de nouvelles découvertes scientifiques\"\n",
    "titre_2 = \"La vie extraordinaire de Marie Curie\"\n",
    "\n",
    "synopsis_3 = \"Sophie est une jeune femme ambitieuse qui travaille dans une grande entreprise. Elle est passionnée par son travail et sa vie sentimentale est au second plan. Un jour, elle rencontre Jake, un artiste bohème et charismatique. Malgré leurs différences, ils sont attirés l'un par l'autre et commencent une relation. Leur amour est intense et passionné, mais leur relation est mise à rude épreuve par les différences de leurs vies. Sophie doit naviguer dans le monde des arts de Jake, qui est très différent de son monde d'affaires. Et Jake doit faire face aux attentes de Sophie en matière de stabilité et de sécurité financière. Alors que leur relation devient de plus en plus sérieuse, ils doivent faire face à des défis imprévus qui mettent leur amour à l'épreuve. Ils doivent trouver un équilibre entre leurs vies et leurs rêves, tout en restant fidèles l'un à l'autre. Le film montre comment l'amour peut être imprévisible et inattendu, et comment il peut nous amener à repenser nos priorités et nos choix de vie. Les personnages principaux sont confrontés à des choix difficiles, mais ils sont prêts à tout pour leur amour. Le film est rempli de scènes romantiques, de moments d'humour et d'émotion, et offre une belle réflexion sur la vie et l'amour.\"\n",
    "titre_3 = \"Un Amour Inattendu\"\n",
    "\n",
    "synopsis_4 = \"Un groupe d'amis décide de passer un week-end dans une vieille maison isolée dans les bois. Mais dès leur arrivée, ils commencent à ressentir une étrange présence et à entendre des bruits inexpliqués. Bientôt, ils se rendent compte que la maison est hantée par des esprits maléfiques qui cherchent à les effrayer et à les tuer.Les amis tentent de quitter la maison, mais ils se retrouvent pris au piège par des phénomènes paranormaux terrifiants. Ils découvrent des indices sur le passé sombre de la maison et sur les esprits qui la hantent. Mais plus ils enquêtent, plus les esprits deviennent violents. Les amis doivent alors faire face à leurs peurs les plus profondes et unir leurs forces pour survivre aux attaques des esprits maléfiques. Ils se rendent compte que pour échapper à la maison hantée, ils doivent résoudre le mystère de sa présence et vaincre les esprits maléfiques. Le film est rempli de scènes effrayantes et surnaturelles, de suspense et de tension. Les personnages sont confrontés à des situations de plus en plus terrifiantes et doivent faire face à leurs propres faiblesses pour espérer s'en sortir. Le suspense et l'horreur sont maintenus jusqu'à la fin du film, offrant un frissonnant voyage dans l'inconnu.\"\n",
    "titre_4 = \"La Maison Hantée\"\n",
    "\n",
    "# create a dataframe with the two new movie with two columns, synopsis and title\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"synopsis\"] = [synopsis, synopsis_2, synopsis_3, synopsis_4]\n",
    "df_new[\"titre\"] = [titre, titre_2, titre_3, titre_4]\n",
    "\n",
    "# process the dataframe\n",
    "df_new = process_df(df_new, train = False)\n",
    "\n",
    "# predict the genre\n",
    "bow_X_new = vectorizer.transform(df_new[\"synopsis_title\"])\n",
    "y_pred_new = model.predict(bow_X_new)\n",
    "\n",
    "print(\"Predicted genres : \", y_pred_new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model has a score of 0.49. For a first model, it is not bad. We will work with this model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the model on some custom data and it seems to work well. It seems that the synopsis needs to be long enough to be able to predict the genre of the movie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Approach\n",
    "\n",
    "We will now use `transformers` to vectorize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = sorted(df_train[\"genre\"].unique())\n",
    "id_to_genre = {i:genre for i, genre in enumerate(genres)}\n",
    "genre_to_id = {genre:i for i, genre in enumerate(genres)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 8\n",
    "scale = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  visit a celebr detect belg embarqu orient expr...      6\n",
      "1  jeun homm origin modest accus meurtr per risqu...      3\n",
      "2  lorsqu mer quatr jeun enfant apprend brutal ca...      3\n",
      "3  vagabond eprend bel jeun vendeux fleur aveugl ...      7\n",
      "4  histoir vrai premi afro americain a avoir inte...      0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Features, Value, ClassLabel, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "\n",
    "data_to_use = pd.DataFrame()\n",
    "data_to_use['text'] = df_train.synopsis_title\n",
    "data_to_use['label'] = df_train.genre.map(genre_to_id)\n",
    "\n",
    "print(data_to_use.head())\n",
    "\n",
    "\n",
    "genre_features = Features({'text': Value('string'), \n",
    "                              'label': ClassLabel(names=genres)})\n",
    "\n",
    "data = Dataset.from_pandas(data_to_use, features=genre_features)\n",
    "data = data.train_test_split(test_size=0.2, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['biopic', 'comédie', 'documentaire', 'drame', 'historique', 'horreur', 'policier', 'romance', 'science fiction'], id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'fil magistrat grand avocat revient petit vill enfanc per a revu depuis longtemp soupcon meurtr decid alor men enquet decouvr verit chemin fais renou famill laquel pris distanc jug',\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "model_name = \"camembert-base\"\n",
    "\n",
    "# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 970, 21839, 221, 6146, 2151, 226, 1674, 4303, 22, 362, 6037, 2536, 33, 15566, 176, 493, 22494, 3438, 286, 1349, 10229, 81, 8, 15830, 33, 5655, 14498, 22, 3452, 8, 6512, 10206, 3762, 312, 1111, 828, 4563, 308, 3385, 215, 4303, 13, 8664, 523, 18, 17742, 216, 76, 4419, 6], [5, 7469, 1724, 4152, 1378, 33, 217, 17168, 528, 1375, 610, 22, 310, 3239, 961, 330, 216, 4789, 2566, 18, 236, 128, 392, 219, 289, 13833, 66, 1867, 7469, 2346, 1375, 22196, 1719, 216, 128, 17937, 7215, 1113, 1016, 11923, 32, 13023, 4443, 111, 573, 215, 1724, 4152, 1378, 6, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(data['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b64a05add24f188ec3ac2c03ac4a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb9070a78d94436b74f4fd6dec868f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁fil', '▁magistrat', '▁grand', '▁avocat', '▁revient', '▁petit', '▁v', 'ill', '▁en', 'f', 'anc', '▁per', '▁a', '▁revu', '▁depuis', '▁long', 'temp', '▁sou', 'p', 'con', '▁meurt', 'r', '▁de', 'cid', '▁a', 'lor', '▁men', '▁en', 'quet', '▁de', 'cou', 'vr', '▁ver', 'it', '▁chemin', '▁fais', '▁ren', 'ou', '▁fa', 'm', 'ill', '▁la', 'quel', '▁pris', '▁d', 'istan', 'c', '▁j', 'ug', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "fil magistrat grand avocat revient petit vill enfanc per a revu depuis longtemp soupcon meurtr decid alor men enquet decouvr verit chemin fais renou famill laquel pris distanc jug\n"
     ]
    }
   ],
   "source": [
    "# Affichage des tokens. XML-Roberta utilise l'algorithm BPE (Byte Pair Encoding) pour encoder les mots\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][0]['input_ids'])\n",
    "print(tokens)\n",
    "print(tokenized_datasets['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32005"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32e40edacc348f083f0d431113e7dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\elmah\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# empty the GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(genres), id2label=id_to_genre, label2id=genre_to_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4a6b67a41c4b5f9d5a4765e84fd0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41103c73bf34c809991ffce0fce1675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8145346641540527, 'eval_accuracy': 0.3791304347826087, 'eval_runtime': 6.4506, 'eval_samples_per_second': 89.139, 'eval_steps_per_second': 11.162, 'epoch': 1.0}\n",
      "{'loss': 1.9188, 'learning_rate': 1.4212962962962964e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0161f60dfa4663a1e72962b237dbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6179980039596558, 'eval_accuracy': 0.46608695652173915, 'eval_runtime': 6.4766, 'eval_samples_per_second': 88.782, 'eval_steps_per_second': 11.117, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbd0416be8a47c1a9f34abdad80964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5553112030029297, 'eval_accuracy': 0.4608695652173913, 'eval_runtime': 6.9503, 'eval_samples_per_second': 82.73, 'eval_steps_per_second': 10.359, 'epoch': 3.0}\n",
      "{'loss': 1.477, 'learning_rate': 8.425925925925926e-06, 'epoch': 3.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d00ecb6c3c4b8aa552f3b240911926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4899864196777344, 'eval_accuracy': 0.4817391304347826, 'eval_runtime': 6.509, 'eval_samples_per_second': 88.339, 'eval_steps_per_second': 11.062, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a867f641594ab4940e098d48b93d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4527311325073242, 'eval_accuracy': 0.5113043478260869, 'eval_runtime': 6.5155, 'eval_samples_per_second': 88.251, 'eval_steps_per_second': 11.05, 'epoch': 5.0}\n",
      "{'loss': 1.2158, 'learning_rate': 2.6388888888888893e-06, 'epoch': 5.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4082ced9b53340f194d1a71762ce19a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4468872547149658, 'eval_accuracy': 0.5252173913043479, 'eval_runtime': 6.5249, 'eval_samples_per_second': 88.124, 'eval_steps_per_second': 11.035, 'epoch': 6.0}\n",
      "{'train_runtime': 664.2127, 'train_samples_per_second': 20.776, 'train_steps_per_second': 2.602, 'train_loss': 1.4784213348671242, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1728, training_loss=1.4784213348671242, metrics={'train_runtime': 664.2127, 'train_samples_per_second': 20.776, 'train_steps_per_second': 2.602, 'train_loss': 1.4784213348671242, 'epoch': 6.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbbc32decc447da95dbba8b94213ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'policier', 'score': 0.7271738648414612}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output = trainer.predict(tokenized_datasets['test'])\n",
    "\n",
    "# compute the accuracy\n",
    "preds = np.argmax(preds_output.predictions, axis=1)\n",
    "labels = preds_output.label_ids\n",
    "accuracy_score(labels, preds)\n",
    "\n",
    "# create a pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# lets try with a new synopsis\n",
    "synopsis = \"Dans la ville de New York, le détective John Smith est sur la piste d'un tueur en série qui terrorise la ville depuis plusieurs mois. Alors que l'enquête piétine, le meurtrier continue de sévir et les victimes s'accumulent. John, qui est hanté par ses propres démons, est déterminé à arrêter le tueur avant qu'il ne frappe à nouveau. Au fil de l'enquête, John rencontre plusieurs personnages suspects, dont une ex-petite amie du tueur et un membre influent de la mafia locale. Il doit naviguer dans un monde sombre et dangereux pour découvrir la vérité sur l'identité du tueur. Alors que les indices s'accumulent et que la pression monte, John doit faire face à ses propres démons intérieurs et aux doutes de ses collègues. Dans une course contre la montre, il doit résoudre le mystère avant qu'il ne soit trop tard et que le tueur ne frappe de nouveau. Le film est rempli de suspense, de tension et de rebondissements jusqu'à son dénouement final choquant.\"\n",
    "titre = \"Au coeur de la nuit\"\n",
    "\n",
    "synopsis_2 = \"Nous suivons Marie Curie dès ses débuts en Pologne, alors qu'elle doit lutter contre les préjugés de genre pour poursuivre ses études scientifiques. Elle rencontre ensuite Pierre Curie, un physicien français, et ils tombent amoureux. Ensemble, ils commencent à travailler sur la radioactivité et découvrent le radium et le polonium, pour lesquels ils reçoivent le prix Nobel de physique en 1903. Mais leur travail n'a pas été facile et ils ont dû surmonter de nombreux obstacles, notamment les préjugés de genre et les réticences de la communauté scientifique. Ils ont également dû faire face à des tragédies personnelles, notamment la mort prématurée de Pierre. Après la mort de Pierre, Marie poursuit ses recherches et devient la première femme professeure à la Sorbonne. Elle reçoit également un deuxième prix Nobel, cette fois en chimie, pour son travail sur les éléments radioactifs. Le film montre comment Marie Curie a changé la face de la science et a ouvert la voie à d'autres femmes scientifiques. Mais il montre également les sacrifices qu'elle a dû faire pour y parvenir, notamment la lutte pour concilier son travail et sa vie de famille. Le film est une célébration de la vie extraordinaire de Marie Curie, une femme qui a brisé les barrières et ouvert la voie à de nouvelles découvertes scientifiques\"\n",
    "titre_2 = \"La vie extraordinaire de Marie Curie\"\n",
    "\n",
    "synopsis_3 = \"Sophie est une jeune femme ambitieuse qui travaille dans une grande entreprise. Elle est passionnée par son travail et sa vie sentimentale est au second plan. Un jour, elle rencontre Jake, un artiste bohème et charismatique. Malgré leurs différences, ils sont attirés l'un par l'autre et commencent une relation. Leur amour est intense et passionné, mais leur relation est mise à rude épreuve par les différences de leurs vies. Sophie doit naviguer dans le monde des arts de Jake, qui est très différent de son monde d'affaires. Et Jake doit faire face aux attentes de Sophie en matière de stabilité et de sécurité financière. Alors que leur relation devient de plus en plus sérieuse, ils doivent faire face à des défis imprévus qui mettent leur amour à l'épreuve. Ils doivent trouver un équilibre entre leurs vies et leurs rêves, tout en restant fidèles l'un à l'autre. Le film montre comment l'amour peut être imprévisible et inattendu, et comment il peut nous amener à repenser nos priorités et nos choix de vie. Les personnages principaux sont confrontés à des choix difficiles, mais ils sont prêts à tout pour leur amour. Le film est rempli de scènes romantiques, de moments d'humour et d'émotion, et offre une belle réflexion sur la vie et l'amour.\"\n",
    "titre_3 = \"Un Amour Inattendu\"\n",
    "\n",
    "synopsis_4 = \"Un groupe d'amis décide de passer un week-end dans une vieille maison isolée dans les bois. Mais dès leur arrivée, ils commencent à ressentir une étrange présence et à entendre des bruits inexpliqués. Bientôt, ils se rendent compte que la maison est hantée par des esprits maléfiques qui cherchent à les effrayer et à les tuer.Les amis tentent de quitter la maison, mais ils se retrouvent pris au piège par des phénomènes paranormaux terrifiants. Ils découvrent des indices sur le passé sombre de la maison et sur les esprits qui la hantent. Mais plus ils enquêtent, plus les esprits deviennent violents. Les amis doivent alors faire face à leurs peurs les plus profondes et unir leurs forces pour survivre aux attaques des esprits maléfiques. Ils se rendent compte que pour échapper à la maison hantée, ils doivent résoudre le mystère de sa présence et vaincre les esprits maléfiques. Le film est rempli de scènes effrayantes et surnaturelles, de suspense et de tension. Les personnages sont confrontés à des situations de plus en plus terrifiantes et doivent faire face à leurs propres faiblesses pour espérer s'en sortir. Le suspense et l'horreur sont maintenus jusqu'à la fin du film, offrant un frissonnant voyage dans l'inconnu.\"\n",
    "titre_4 = \"La Maison Hantée\"\n",
    "\n",
    "# create a dataframe with the two new movie with two columns, synopsis and title\n",
    "df_new = pd.DataFrame()\n",
    "df_new[\"synopsis\"] = [synopsis, synopsis_2, synopsis_3, synopsis_4]\n",
    "df_new[\"titre\"] = [titre, titre_2, titre_3, titre_4]\n",
    "\n",
    "\n",
    "# process the dataframe\n",
    "df_new = process_df(df_new, train = False)\n",
    "\n",
    "\n",
    "# predict the genre\n",
    "classifier(df_new[\"synopsis_title\"][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# now test the classifier on the test set\n",
    "df_test = pd.read_csv(\"../data/allocine_genres_test.csv\")\n",
    "\n",
    "# process the dataframe\n",
    "df_test = process_df(df_test, train = True)\n",
    "\n",
    "# predict the genre\n",
    "df_test[\"predicted_genre\"] = df_test[\"synopsis_title\"].apply(classifier)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test.predicted_genre[0][0]\n",
    "\n",
    "# extract genre from dict \n",
    "def extract_genre(genre_dict):\n",
    "    # dict is under this form {'label': 'drame', 'score': 0.4321644604206085}\n",
    "    return genre_dict[0][\"label\"]\n",
    "\n",
    "# apply the function to the predicted genre\n",
    "df_test[\"predicted_genre\"] = df_test[\"predicted_genre\"].apply(extract_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5257301808066759"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()\n",
    "\n",
    "# check the accuracy\n",
    "accuracy_score(df_test[\"genre\"], df_test[\"predicted_genre\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We seem to have a better score with this approach. We will use this model to predict the genre of the movies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
